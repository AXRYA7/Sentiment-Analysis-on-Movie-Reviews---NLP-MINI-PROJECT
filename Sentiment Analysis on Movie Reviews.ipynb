{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddf644f-225f-470d-85d7-6f89098deb19",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis on Movie Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb004b3-4a44-4a52-aa4c-1a2fab8d47fa",
   "metadata": {},
   "source": [
    "### Importing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b41aca-8d48-43b1-b4c5-ba02853c9b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "Index(['review', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Loading the dataset\n",
    "data = pd.read_csv(\"Datasets/dataset/archive/IMDB Dataset.csv\")\n",
    "\n",
    "#Checking the first few rows \n",
    "print(data.head())\n",
    "\n",
    "#Checking the columns\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab3172-612e-40d6-8f20-0571681d8e06",
   "metadata": {},
   "source": [
    "### Pre-processing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf81ee7-2075-4a33-9264-fa499c1d44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aarya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aarya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aarya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review : /n This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\n",
      "\n",
      " After Cleaning : \n",
      " ['show', 'amaze', 'fresh', 'innovative', 'idea', 'first', 'air', 'first', 'years', 'brilliant', 'things', 'drop', 'show', 'really', 'funny', 'anymore', 'continue', 'decline', 'complete', 'waste', 'time', 'todayits', 'truly', 'disgraceful', 'far', 'show', 'fall', 'write', 'painfully', 'bad', 'performances', 'almost', 'bad', 'mildly', 'entertain', 'respite', 'guesthosts', 'show', 'probably', 'wouldnt', 'still', 'air', 'find', 'hard', 'believe', 'creator', 'handselected', 'original', 'cast', 'also', 'choose', 'band', 'hack', 'follow', 'one', 'recognize', 'brilliance', 'see', 'fit', 'replace', 'mediocrity', 'felt', 'must', 'give', 'star', 'respect', 'original', 'cast', 'make', 'show', 'huge', 'success', 'show', 'awful', 'cant', 'believe', 'still', 'air']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Downloading the stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "#Downloading the Dictionary for Lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Load the datasets\n",
    "data = pd.read_csv(\"Datasets/dataset/archive/IMDB Dataset.csv\")\n",
    "\n",
    "#Example Review\n",
    "review = data['review'][7]\n",
    "print(\"Original Review : /n\", review)\n",
    "\n",
    "#1. Preprocessing (Reducing the example dataset to lowercase) \n",
    "review = review.lower()\n",
    "\n",
    "#2. Removing the HTML tags i.e (br /)\n",
    "review = re.sub(r'<.*?>', '', review)\n",
    "\n",
    "#3. Remove Punctuations/Numbers\n",
    "review = re.sub(r'[^a-z\\s]', '', review)\n",
    "\n",
    "#4. Tokenization\n",
    "tokens = review.split()\n",
    "\n",
    "#5. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "#6. Lemmatization\n",
    "lemmatizer =  WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "\n",
    "print(\"\\n After Cleaning : \\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9f156-a1bb-4a9f-903e-cb39cb543193",
   "metadata": {},
   "source": [
    "### Preprocessing function - applying preprocessing for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe5ed17-a8f7-4d87-9f88-d994cd582b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "\n",
      "                                        clean_review  \n",
      "0  one reviewers mention watch oz episode youll h...  \n",
      "1  wonderful little production film technique una...  \n",
      "\n",
      " It Works !\n"
     ]
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# initializing the Lemmatizer and the stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Defining the clean_review Function for cleaning the dataset\n",
    "def clean_review(text):\n",
    "    #1. Convert all the the data in the dataset into lowercase()\n",
    "    text = text.lower()\n",
    "\n",
    "    #2. removing HTML tags from the dataset \n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    #3. Removing the Punctuations and numbers from the dataset \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    #4 Tokenize the dataset\n",
    "    tokens = text.split()\n",
    "\n",
    "    #5. Remove the stopwords from the dataset \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    #6. Lemmatizing the dataset for better accuracy\n",
    "    tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "\n",
    "    # Prints the Clean_review as a single string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv(\"Datasets/dataset/archive/IMDB Dataset.csv\")\n",
    "\n",
    "# applying the function to the complete dataset\n",
    "data ['clean_review'] = data ['review'].apply(clean_review)\n",
    "\n",
    "# Checking whether the function works properly \n",
    "print(data[['review', 'clean_review']].head(2))\n",
    "\n",
    "#YAYYYY IT WORKSS !!!!!\n",
    "print(\"\\n It Works !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87f5a6-0297-49b1-a657-98425214b951",
   "metadata": {},
   "source": [
    "### Feature Extraction For ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "febd9e60-bd02-4cc3-a47b-98c6fefbce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Of TF_IDF Matrix (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "'''Using SKLEARN library\n",
    "    sklearn.feature_extraction.text → has functions to turn text → numbers.\n",
    "    TfidfVectorizer → specifically makes the TF-IDF matrix.'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Creating a TF-IDF object\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Applying TF_IDF on all reviews\n",
    "x = vectorizer.fit_transform(data['clean_review'])\n",
    "\n",
    "# Preparing the labels (what we want to predict)\n",
    "y = data['sentiment'].map({'positive' : 1, 'negative' : 0})\n",
    "\n",
    "print(\"Shape Of TF_IDF Matrix\", x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
